{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0a5ea72841f3384e9270b0a054395eca0d7b72480707ef224b05b46ae15c777a0",
   "display_name": "Python 3.8.5 64-bit ('cs7643-a2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You are using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/rgur/courses/cs_7643_deep_learning/hw4/')\n",
    "\n",
    "# Just run this block. Please do not modify the following code.\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Pytorch package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Torchtest package\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, Example, TabularDataset, interleave_keys, LabelField\n",
    "\n",
    "# Tqdm progress bar\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "# Code provide to you for training and evaluation\n",
    "#from hw4_code.utils import train, evaluate, set_seed_nb, unit_test_values\n",
    "from utils import qe_train, qe_evaluate\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# You don't need to modify any code in this block\n",
    "\n",
    "# Define the maximum length of the sentence. Shorter sentences will be padded to that length and longer sentences will be croped. Given that the average length of the sentence in the corpus is around 13, we can set it to 20\n",
    "MAX_LEN = 20\n",
    "\n",
    "# Define the source and target language\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            fix_length = MAX_LEN,\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            fix_length = MAX_LEN,\n",
    "            lower = True)\n",
    "\n",
    "Z = LabelField(dtype=torch.float, batch_first=True, use_vocab=False)\n",
    "\n",
    "train_data = TabularDataset(path='/data/rgur/courses/cs_7643_deep_learning/project/mlqe/data/en-de/train.ende.df.short.tsv',format='TSV',fields={'original':('src',SRC),\n",
    "'translation':('trg',TRG),\n",
    "'z_mean':('z',Z)})\n",
    "\n",
    "val_data = TabularDataset(path='/data/rgur/courses/cs_7643_deep_learning/project/mlqe/data/en-de/dev.ende.df.short.tsv',format='TSV',fields={'original':('src',SRC),\n",
    "'translation':('trg',TRG),\n",
    "'z_mean':('z',Z)})\n",
    "\n",
    "# Define Batchsize\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Build the vocabulary associated with each language\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "# Get the padding index to be ignored later in loss calculation\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "# Get data-loaders using BucketIterator\n",
    "train_loader = BucketIterator( #Defines an iterator that batches examples of similar lengths together.\n",
    "    train_data,\n",
    "    batch_size = BATCH_SIZE, device = device, sort_key=lambda x: interleave_keys(len(x.src), len(x.trg)))\n",
    "\n",
    "val_loader = BucketIterator( #Defines an iterator that batches examples of similar lengths together.\n",
    "    val_data,\n",
    "    batch_size = BATCH_SIZE, device = device, sort_key=lambda x: interleave_keys(len(x.src), len(x.trg)))\n",
    "\n",
    "# Get the input and the output sizes for model\n",
    "input_size = len(SRC.vocab)\n",
    "output_size = len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-layer Transformer which encodes a sequence of text and \n",
    "    performs binary classification.\n",
    "\n",
    "    The model has a vocab size of V, works on\n",
    "    sequences of length T, has an hidden dimension of H, uses word vectors\n",
    "    also of dimension H, and operates on minibatches of size N.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=256*20, p=0):\n",
    "        super(QEModel, self).__init__()\n",
    "        self.lin1 = nn.Linear(256*20,1028)\n",
    "        self.lin2 = nn.Linear(1028,1)\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu( self.lin1(x) )\n",
    "        x = nn.functional.dropout(x,p=self.p)\n",
    "        return self.lin2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qe_model = QEModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(hw4_code.models.Transformer)\n",
    "from hw4_code.models.Transformer import TransformerTranslator\n",
    "source_model = TransformerTranslator(output_size, input_size, device, max_length = MAX_LEN).to(device)\n",
    "target_model = TransformerTranslator(input_size, output_size, device, max_length = MAX_LEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "source_model.load_state_dict(torch.load('/data/rgur/courses/cs_7643_deep_learning/project/de_en.pt'))\n",
    "target_model.load_state_dict(torch.load('/data/rgur/courses/cs_7643_deep_learning/project/en_de.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nlearning_rate 0.001\nP 0.2 \n\n-----------------------------------\nEpoch 1\n-----------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/55 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b7a913a4225435da62a617325855369"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56103f679c67459db3cccb8bd8cdce40"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preds: tensor([-0.1625, -0.1037, -0.1287, -0.3031, -0.1421], device='cuda:0',\n       grad_fn=<SliceBackward>)\nlabel: tensor([ 0.0889, -0.1252,  0.4844,  0.5740, -0.3063], device='cuda:0')\nTraining Loss: 0.4097. Validation RMSE: 0.8748. Validation R2: -0.3810. Validation MAE 0.6548\n-----------------------------------\nEpoch 2\n-----------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/55 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9411a9e55ca043faa9be8131cf617c34"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc46155841e449a0858b0eb538fc1c88"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preds: tensor([ 0.0243,  0.8284, -0.1087,  0.0220, -0.4120], device='cuda:0',\n       grad_fn=<SliceBackward>)\nlabel: tensor([-3.7308, -0.2132,  0.4478, -0.3509, -1.8152], device='cuda:0')\nTraining Loss: 0.2954. Validation RMSE: 0.8845. Validation R2: -0.4068. Validation MAE 0.6750\n-----------------------------------\nEpoch 3\n-----------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/55 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dbecde3cec24720821c440f6cfb1694"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "febabe12dc7d41749fa065228cc2b3a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preds: tensor([ 0.0513, -0.6178, -0.6755,  0.4259, -0.4045], device='cuda:0',\n       grad_fn=<SliceBackward>)\nlabel: tensor([0.2813, 0.3807, 0.0965, 0.3739, 0.5116], device='cuda:0')\nTraining Loss: 0.2845. Validation RMSE: 0.8851. Validation R2: -0.4082. Validation MAE 0.6743\n-----------------------------------\nEpoch 4\n-----------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/55 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "577582c887794bba8d7a69c28803008a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f278bf8bbb914f819708bd407a3bc053"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preds: tensor([ 0.5836,  0.5232, -0.3560,  0.2150,  1.0562], device='cuda:0',\n       grad_fn=<SliceBackward>)\nlabel: tensor([ 0.6900, -0.2491, -0.4016,  0.3875,  0.5593], device='cuda:0')\nTraining Loss: 0.2660. Validation RMSE: 0.8845. Validation R2: -0.4030. Validation MAE 0.6665\n-----------------------------------\nEpoch 5\n-----------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/55 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c9a0efd1ed9418bbed9bfcb7241a5a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00427d57d6534ecfa981dc2cc7b476c1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preds: tensor([ 0.0247,  0.4269,  0.1956, -0.0909,  0.3991], device='cuda:0',\n       grad_fn=<SliceBackward>)\nlabel: tensor([ 0.6050,  0.1806, -0.1087, -0.9230,  0.6749], device='cuda:0')\nTraining Loss: 0.2548. Validation RMSE: 0.9084. Validation R2: -0.4822. Validation MAE 0.6779\n-----------------------------------\nEpoch 6\n-----------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/55 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1ff59e7b7bf4afba1f26ef46186e5f5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e17edc7f014b4ba682b764536074a279"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preds: tensor([-1.6143,  0.6347,  0.5045,  0.3073, -0.4980], device='cuda:0',\n       grad_fn=<SliceBackward>)\nlabel: tensor([ 0.7686,  0.5842,  0.6759, -0.8717,  0.8283], device='cuda:0')\nTraining Loss: 0.2430. Validation RMSE: 0.9009. Validation R2: -0.4617. Validation MAE 0.6731\n-----------------------------------\nEpoch 7\n-----------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/55 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85c18b1d61fa4ca0af89408ccdb2a8f5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1ff6ff676ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqe_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqe_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/courses/cs_7643_deep_learning/project/utils.py\u001b[0m in \u001b[0;36mqe_train\u001b[0;34m(qe_model, source_model, target_model, dataloader, optimizer, criterion, scheduler)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#     print('label:', z[0:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cs7643-a2/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for learning_rate in [,.05, .01, .001, .0001]:\n",
    "    for P in [.2, .3, .5]:\n",
    "        print('\\nlearning_rate', learning_rate)\n",
    "        print('P', P, '\\n')\n",
    "        # Hyperparameters\n",
    "        EPOCHS = 10\n",
    "\n",
    "        # Model\n",
    "\n",
    "\n",
    "        # optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "        optimizer = torch.optim.Adam(qe_model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch_idx in range(EPOCHS):\n",
    "            print(\"-----------------------------------\")\n",
    "            print(\"Epoch %d\" % (epoch_idx+1), flush=True)\n",
    "            print(\"-----------------------------------\")\n",
    "            \n",
    "            train_loss, avg_train_loss = qe_train(qe_model, source_model, target_model, train_loader, optimizer, criterion, scheduler = None)\n",
    "            train_loss = train_loss.item()\n",
    "            scheduler.step(train_loss)\n",
    "\n",
    "            #val_loss, avg_val_loss = evaluate(trans_model, val_loader, criterion)\n",
    "\n",
    "            avg_train_loss = avg_train_loss.item()\n",
    "            val_loss, avg_val_loss, r2, mae = qe_evaluate(qe_model, source_model, target_model, val_loader, criterion)\n",
    "            #avg_val_loss = 0\n",
    "            avg_val_loss = avg_val_loss.item()\n",
    "\n",
    "            print(\"Training Loss: %.4f. Validation RMSE: %.4f. Validation R2: %.4f. Validation MAE %.4f\" % (np.sqrt(avg_train_loss), np.sqrt(avg_val_loss), r2, mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}